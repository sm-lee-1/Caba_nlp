{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "npl10_Text Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP01Jo0A+ySmmaBW7VDWU0g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sm-lee-1/Caba_nlp/blob/main/npl10_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ezrRl7C7Gez"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8h8Vfbz7Q2O"
      },
      "source": [
        "NLP, 텍스트 분석\r\n",
        "- Natural Language Processing : 기계가 인간의 언어를 이해하고 해석하는데 중점. 기계번역, 질의응답시스템\r\n",
        "- 텍스트 분석 : 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점\r\n",
        "- NLP는 텍스트 분석을 향상하게 하는 기반 기술\r\n",
        "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과저 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\r\n",
        "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\r\n",
        "    - 텍스트 분류 : 신문기사 카테고리 분류, 스팸 메일 검출 프로그램. 지도학습\r\n",
        "    - 감성 분석 : 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화리뷰, 여론조사 의견분석. 지도학습, 비지도학습\r\n",
        "    - 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\r\n",
        "    - 텍스트 군집화화 유사도 측정 : 비슷한 유형의 문서에 대해 군집화 수행. 비지도 학습\r\n",
        "\r\n",
        "Text 분석 수행 프로세스\r\n",
        "- 텍스트 정규화    \r\n",
        "    - 클랜징, 토큰화, 필터링/스톱워드 제거/철자 수정, Stemming, Lemmatization\r\n",
        "- 피처 벡터화 변환\r\n",
        "    - Bag of Words : Count 기반, TF-IDF 기반\r\n",
        "    - Word2Vec\r\n",
        "- ML 모델 수립 및 학습/예측/평가\r\n",
        "\r\n",
        "텍스트 전처리 - 텍스트 정규화\r\n",
        "- 클렌징 : 분석에 방해되는 불필요한 문자, 기호를 사전에 제거. HTML, XML 태그나 특정 기호\r\n",
        "- 토큰화 : 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화\r\n",
        "- 필터링/스톱워드 제거/철자 수정 : 분석에 큰 의미가 없는 단어를 제거\r\n",
        "- Stemming, Lemmatization : 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음\r\n",
        "    - Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 찾음\r\n",
        "    - Lemmatization이 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C1-Q2gE_crd",
        "outputId": "eea9686d-4273-41c2-8166-bdf82f9fbce6"
      },
      "source": [
        "# 영어 분석을 위함\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVG_6508DRgp",
        "outputId": "f87aa7a8-6f62-426f-a956-b3a11c20f971"
      },
      "source": [
        "# 문장 토큰화(sent tokenize)\r\n",
        "from nltk import sent_tokenize\r\n",
        "text_sample = \"If you want to solve a particular kind of business problem with machine learning, \\\r\n",
        "you’ll likely have no trouble finding a tutorial showing you how to extract features and train a model. \\\r\n",
        "However, building machine learning systems isn’t just about training models or even about finding the best features; \\\r\n",
        "if you want a blueprint for a real system or want to see how to address more of the data science workflow, \\\r\n",
        "many tutorials leave the hard parts as an exercise for the reader.\"\r\n",
        "sentences = sent_tokenize(text=text_sample)\r\n",
        "print(sentences)\r\n",
        "print(type(sentences), len(sentences))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['If you want to solve a particular kind of business problem with machine learning, you’ll likely have no trouble finding a tutorial showing you how to extract features and train a model.', 'However, building machine learning systems isn’t just about training models or even about finding the best features; if you want a blueprint for a real system or want to see how to address more of the data science workflow, many tutorials leave the hard parts as an exercise for the reader.']\n",
            "<class 'list'> 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yg-a-5I-EVv-",
        "outputId": "f79b6981-e174-4992-9d85-eeb2b3056e20"
      },
      "source": [
        "# 단어 토큰화(word_tokenize) :  공백, 콤마, 마침표, 개행문자, 정규표현식\r\n",
        "from nltk import word_tokenize\r\n",
        "\r\n",
        "sentences = 'If you want to solve a particular kind of business problem with machine learning'\r\n",
        "words = word_tokenize(sentences)\r\n",
        "print(words)\r\n",
        "print(type(words), len(words))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['If', 'you', 'want', 'to', 'solve', 'a', 'particular', 'kind', 'of', 'business', 'problem', 'with', 'machine', 'learning']\n",
            "<class 'list'> 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtjbQZcQFCNZ",
        "outputId": "646f580e-7574-4f8e-f8d1-fe3ae5df4de6"
      },
      "source": [
        "from nltk import sent_tokenize\r\n",
        "from nltk import word_tokenize\r\n",
        "\r\n",
        "def tokenize_text(text):\r\n",
        "  sentences = sent_tokenize(text=text)  # 문장별 분리 토큰\r\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences]  # 문장별 단어 토큰화\r\n",
        "  return word_tokens \r\n",
        "\r\n",
        "word_tokens = tokenize_text(text_sample)\r\n",
        "print(word_tokens)\r\n",
        "print(type(word_tokens), len(word_tokens))\r\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['If', 'you', 'want', 'to', 'solve', 'a', 'particular', 'kind', 'of', 'business', 'problem', 'with', 'machine', 'learning', ',', 'you', '’', 'll', 'likely', 'have', 'no', 'trouble', 'finding', 'a', 'tutorial', 'showing', 'you', 'how', 'to', 'extract', 'features', 'and', 'train', 'a', 'model', '.'], ['However', ',', 'building', 'machine', 'learning', 'systems', 'isn', '’', 't', 'just', 'about', 'training', 'models', 'or', 'even', 'about', 'finding', 'the', 'best', 'features', ';', 'if', 'you', 'want', 'a', 'blueprint', 'for', 'a', 'real', 'system', 'or', 'want', 'to', 'see', 'how', 'to', 'address', 'more', 'of', 'the', 'data', 'science', 'workflow', ',', 'many', 'tutorials', 'leave', 'the', 'hard', 'parts', 'as', 'an', 'exercise', 'for', 'the', 'reader', '.']]\n",
            "<class 'list'> 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_pFpTWX7LXj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqXt5vyuH4ZG",
        "outputId": "a5805d21-6db2-4423-d627-bca3379042bc"
      },
      "source": [
        "# 스톱워드 제거 : the, is, a, will 와 같이 문맥적으로 큰 의미가 없는 단어를 제거\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3TYhV_FIKwZ",
        "outputId": "046a3443-1490-41f1-939d-984b74dbd34e"
      },
      "source": [
        "# NLTK english stopwords 갯수 확인\r\n",
        "print(len(nltk.corpus.stopwords.words('english')))\r\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMqIwx3VIK9B",
        "outputId": "e9929015-c11a-48c1-cac5-5c68b6799583"
      },
      "source": [
        "# stopwords 필터링을 통한 제거\r\n",
        "import nltk\r\n",
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "all_tokens = []\r\n",
        "\r\n",
        "for sentence in word_tokens:\r\n",
        "  filtered_words = []\r\n",
        "  for word in sentence:\r\n",
        "    word = word.lower()\r\n",
        "    if word not in stopwords:\r\n",
        "      filtered_words.append(word)\r\n",
        "\r\n",
        "  all_tokens.append(filtered_words)\r\n",
        "\r\n",
        "print(all_tokens)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['want', 'solve', 'particular', 'kind', 'business', 'problem', 'machine', 'learning', ',', '’', 'likely', 'trouble', 'finding', 'tutorial', 'showing', 'extract', 'features', 'train', 'model', '.'], ['however', ',', 'building', 'machine', 'learning', 'systems', '’', 'training', 'models', 'even', 'finding', 'best', 'features', ';', 'want', 'blueprint', 'real', 'system', 'want', 'see', 'address', 'data', 'science', 'workflow', ',', 'many', 'tutorials', 'leave', 'hard', 'parts', 'exercise', 'reader', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmORD3H9KU_Q",
        "outputId": "4a0cbfd7-6100-42cf-fb8d-5efe6cbc56a5"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\r\n",
        "# stemmer(LancasterStemmer)\r\n",
        "from nltk.stem import LancasterStemmer\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\r\n",
        "print(stemmer.stem('amusing'), stemmer.stem('ameses'), stemmer.stem('amused'))\r\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fancier'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus aums amus\n",
            "fant fant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljFrk_8eMnl7",
        "outputId": "99b7e153-cd20-4ecf-da19-838bf1dc954f"
      },
      "source": [
        "import nltk \r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZIU2tDOLN2E",
        "outputId": "2ca58046-5f87-486f-a514-dfb733b94439"
      },
      "source": [
        "# Lemmatization(WordNetLemmatizer) : 정확한 원형 단어 추출을 위해 단어의 품사를 직접 입력\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "\r\n",
        "lemma = WordNetLemmatizer()\r\n",
        "print(lemma.lemmatize('working','v'), lemma.lemmatize('works','v'), lemma.lemmatize('worked','v'))\r\n",
        "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\r\n",
        "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fancier','a'))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amuse aumses amuse\n",
            "fancy fancy\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}